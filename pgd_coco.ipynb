{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c52a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ce6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "image = Image.open(\"coco/images/val2014/COCO_val2014_000000000042.jpg\").convert(\"RGB\")\n",
    "prompt = \"<image>\\nDescribe this image in detail.\"\n",
    "\n",
    "# âœ… Corrected processor call\n",
    "inputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"LLaVA output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01389eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack_image(\n",
    "    model,\n",
    "    processor,\n",
    "    image_pil,\n",
    "    prompt=\"<image>\\nDescribe this image in detail.\",\n",
    "    epsilon=8/255,\n",
    "    alpha=2/255,\n",
    "    num_steps=20,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Final corrected PGD attack on a single image for LLaVA-1.5-7B (HF).\n",
    "    \n",
    "    Args:\n",
    "        model: HuggingFace LLaVA model\n",
    "        processor: Corresponding AutoProcessor\n",
    "        image_pil: Original PIL image\n",
    "        prompt: Prompt string including <image> token\n",
    "        epsilon: Max Lâˆž perturbation (in [0,1] pixel space)\n",
    "        alpha: PGD step size (in [0,1] pixel space)\n",
    "        num_steps: Number of PGD iterations\n",
    "        device: PyTorch device string\n",
    "        \n",
    "    Returns:\n",
    "        adv_pil: Adversarial PIL image\n",
    "        adv_caption: Model's output on adversarial image\n",
    "        clean_caption: Original caption\n",
    "    \"\"\"\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # Step 1: Get clean caption on the original image\n",
    "    with torch.no_grad():\n",
    "        clean_inputs = processor(text=prompt, images=[image_pil], return_tensors=\"pt\").to(device)\n",
    "        clean_output_ids = model.generate(**clean_inputs, max_new_tokens=100)\n",
    "        clean_caption = processor.batch_decode(clean_output_ids, skip_special_tokens=True)[0].strip()\n",
    "        # Post-process to remove the prompt from the model's output\n",
    "        prompt_str_to_remove = prompt.replace('<image>\\n', '')\n",
    "        if clean_caption.startswith(prompt_str_to_remove):\n",
    "             clean_caption = clean_caption[len(prompt_str_to_remove):].strip()\n",
    "\n",
    "    attack_text = prompt + \" \" + clean_caption\n",
    "    \n",
    "    inputs = processor(text=attack_text, images=[image_pil], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    original_pixel_values = inputs['pixel_values'].clone().detach()\n",
    "    adv_pixel_values = original_pixel_values.clone().detach()\n",
    "    adv_pixel_values.requires_grad = True\n",
    "\n",
    "    labels = inputs.input_ids.clone()\n",
    "    prompt_token_len = processor(text=prompt, return_tensors=\"pt\").input_ids.shape[1]\n",
    "    labels[:, :prompt_token_len] = -100\n",
    "\n",
    "    # PGD Attack Loop\n",
    "    for _ in range(num_steps):\n",
    "        model.zero_grad()\n",
    "        attack_inputs = {\n",
    "            \"pixel_values\": adv_pixel_values,\n",
    "            \"input_ids\": inputs.input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "        outputs = model(**attack_inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            grad_sign = adv_pixel_values.grad.sign()\n",
    "            adv_pixel_values.data = adv_pixel_values.data + alpha * grad_sign\n",
    "            perturbation = torch.clamp(adv_pixel_values.data - original_pixel_values, -epsilon, epsilon)\n",
    "            adv_pixel_values.data = original_pixel_values + perturbation\n",
    "            \n",
    "        adv_pixel_values.grad.zero_()\n",
    "\n",
    "\n",
    "    # Step 3: Create final adversarial image and get its caption\n",
    "    image_processor = processor.image_processor\n",
    "    mean = torch.tensor(image_processor.image_mean, device=device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor(image_processor.image_std, device=device).view(1, 3, 1, 1)\n",
    "    \n",
    "    unnormalized_adv_pixels = adv_pixel_values * std + mean\n",
    "    unnormalized_adv_pixels = torch.clamp(unnormalized_adv_pixels, 0, 1)\n",
    "    \n",
    "    adv_np = unnormalized_adv_pixels[0].detach().cpu().permute(1, 2, 0).numpy()\n",
    "    adv_pil = Image.fromarray((adv_np * 255).astype(np.uint8))\n",
    "\n",
    "    adv_pil = adv_pil.resize(image_pil.size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        final_inputs = processor(text=prompt, images=[adv_pil], return_tensors=\"pt\").to(device)\n",
    "        final_output_ids = model.generate(**final_inputs, max_new_tokens=100)\n",
    "        adv_caption = processor.batch_decode(final_output_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    return adv_pil, adv_caption, clean_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"coco/images/val2014/COCO_val2014_000000552842.jpg\").convert(\"RGB\")\n",
    "\n",
    "adv_img_tensor, adv_caption, clean_caption = pgd_attack_image(\n",
    "    model, processor, image,\n",
    "    prompt=\"<image>\\nDescribe this image in detail.\",\n",
    "    epsilon=4/255, alpha=1/255, num_steps=50, device=\"cuda\"\n",
    ")\n",
    "\n",
    "print(\"ðŸŸ¢ Clean Caption:\", clean_caption)\n",
    "print(\"ðŸ”´ Adversarial Caption:\", adv_caption)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
