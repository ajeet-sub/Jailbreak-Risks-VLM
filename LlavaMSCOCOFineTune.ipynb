{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb84049-6f5d-41b6-b33d-f62744e41448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Cell 0: Define Configuration\n",
    "# ------------------------------\n",
    "class Config:\n",
    "    dataset_name = \"coco_caption\"  # or your dataset\n",
    "    max_samples = 10               # limit for testing\n",
    "    image_token = \"<image>\"        # token used in LLaVA prompts\n",
    "\n",
    "config = Config()\n",
    "print(f\"Set!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0d624-b72a-43f1-b16b-d16edbdb423e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0.dev20250319+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.56.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.56.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: peft==0.9.0 in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (4.56.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (1.10.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (0.6.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.9.0) (0.35.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.9.0) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.13.0->peft==0.9.0) (77.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.9.0) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.9.0) (0.22.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.9.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.9.0) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (2025.8.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets==3.6.0 in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.35.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.3.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.4)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (6.32.1)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.9)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.38.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mName: peft\n",
      "Version: 0.9.0\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: sourab@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: accelerate, huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install Required Dependencies\n",
    "\"\"\"\n",
    "Install all necessary packages for LLaVA fine-tuning with DeepSpeed support.\n",
    "We need transformers, deepspeed, peft (for LoRA), and other ML libraries.\n",
    "\"\"\"\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install --upgrade --no-cache-dir transformers sentencepiece\n",
    "!pip install --upgrade transformers sentencepiece\n",
    "!pip install deepspeed>=0.12.0     \n",
    "!pip install peft==0.9.0\n",
    "!pip install accelerate>=0.25.0    \n",
    "!pip install datasets==3.6.0\n",
    "!pip install Pillow>=9.0.0         \n",
    "!pip install requests              \n",
    "!pip install pycocotools           \n",
    "!pip install wandb                 \n",
    "!pip show peft  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351849e-c909-4a6f-aa5d-cc23f3ae132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece version 0.2.1 is compatible ✅\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "required_version = \"0.1.99\"\n",
    "\n",
    "try:\n",
    "    import sentencepiece as spm\n",
    "    from packaging import version\n",
    "    if version.parse(spm.__version__) >= version.parse(required_version):\n",
    "        print(f\"SentencePiece version {spm.__version__} is compatible \")\n",
    "    else:\n",
    "        print(f\"SentencePiece version {spm.__version__} is too old \")\n",
    "except ImportError:\n",
    "    print(\"SentencePiece is not installed ❌\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2eb9a-ddcf-4c76-aefb-0e5854155782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Required Libraries\n",
    "\"\"\"\n",
    "Import all necessary libraries for the fine-tuning process.\n",
    "Each import serves a specific purpose in our pipeline.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,                    \n",
    "    TrainingArguments,              \n",
    "    Trainer,                         \n",
    "    EarlyStoppingCallback            \n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,                      \n",
    "    get_peft_model,                  \n",
    "    TaskType,                        \n",
    "    prepare_model_for_kbit_training  \n",
    ")\n",
    "import deepspeed                     \n",
    "from datasets import load_dataset    \n",
    "import os\n",
    "import json\n",
    "from PIL import Image               \n",
    "import requests                     \n",
    "from typing import Dict, List, Any, Optional\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4912e-1fe5-4d0a-b275-a84a2155a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Training on 1 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Cell 3: Configuration & Hyperparameters\n",
    "# ------------------------------\n",
    "class Config:\n",
    "    \n",
    "    model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "    processor_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "    \n",
    "    dataset_name = \"jxie/coco_captions\"  \n",
    "    max_samples = 1000\n",
    "    image_token = \"<image>\"\n",
    "\n",
    "    \n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.1\n",
    "    lora_target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    "\n",
    "   \n",
    "    output_dir = \"./llava-coco-lora\"\n",
    "    num_train_epochs = 3\n",
    "    per_device_train_batch_size = 2\n",
    "    per_device_eval_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    learning_rate = 2e-4\n",
    "    warmup_steps = 100\n",
    "    logging_steps = 50\n",
    "    save_steps = 500\n",
    "    eval_steps = 500\n",
    "    max_grad_norm = 1.0\n",
    "    dataloader_num_workers = 4\n",
    "\n",
    "config = Config()\n",
    "print(f\"Configuration loaded. Training on {torch.cuda.device_count()} GPU(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa47d2f-7ed5-4604-9db0-1bbf50297c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a1ff4a9ad240a795e1fe6bd3a8b124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00751278e1b43a0859ec528aac0d74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor and model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Load LLaVA Processor and Model\n",
    "# ------------------------------\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "llava_module = importlib.import_module(\"transformers.models.llava.modeling_llava\")\n",
    "LlavaForConditionalGeneration = getattr(llava_module, \"LlavaForConditionalGeneration\")\n",
    "\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Processor and model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2c793-eeb4-40b4-a9b3-7141b8138477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: yerevann/coco-karpathy\n",
      "Dataset loaded successfully!\n",
      "Error creating dataset: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "Please check the dataset format and try again.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "config.dataset_name = \"yerevann/coco-karpathy\" \n",
    "config.max_samples = None\n",
    "\n",
    "\n",
    "print(f\"Loading dataset: {config.dataset_name}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    dataset_stream = load_dataset(config.dataset_name, split=\"train\", streaming=True)\n",
    "    dataset_stream = dataset_stream.take(config.max_samples)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    # Fallback options\n",
    "    print(\"Trying alternative dataset...\")\n",
    "    try:\n",
    "        config.dataset_name = \"HuggingFaceM4/COCO\"\n",
    "        dataset_stream = load_dataset(config.dataset_name, split=\"train\", streaming=True)\n",
    "        dataset_stream = dataset_stream.take(config.max_samples)\n",
    "        print(\"Alternative dataset loaded!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Alternative also failed: {e2}\")\n",
    "        print(\"You may need to use a different dataset or check your internet connection\")\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, dataset_stream, processor, max_length=512, max_samples=None):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "        \n",
    "        if max_samples is not None:\n",
    "            self.dataset = []\n",
    "            for i, example in enumerate(dataset_stream):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                self.dataset.append(example)\n",
    "        else:\n",
    "            self.dataset = list(dataset_stream)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        \n",
    "       \n",
    "        try:\n",
    "            if isinstance(sample.get('image'), Image.Image):\n",
    "                image = sample['image']\n",
    "            elif 'url' in sample:\n",
    "                image = Image.new('RGB', (224, 224), color='white')\n",
    "                print(f\"Using placeholder image for sample {idx} (URL: {sample.get('url', 'N/A')})\")\n",
    "            else:\n",
    "                image = Image.open(sample['image']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            \n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "        \n",
    "    \n",
    "        if 'caption' in sample:\n",
    "            caption = sample['caption']\n",
    "        elif 'sentences' in sample and len(sample['sentences']) > 0:\n",
    "          \n",
    "            caption = sample['sentences'][0]\n",
    "        elif 'text' in sample:\n",
    "            caption = sample['text']\n",
    "        else:\n",
    "           \n",
    "            caption = \"A photo.\"\n",
    "            print(f\"No caption found for sample {idx}, using fallback\")\n",
    "        \n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": f\"{config.image_token}\\nDescribe this image.\"},\n",
    "            {\"role\": \"assistant\", \"content\": caption}\n",
    "        ]\n",
    "        \n",
    "        text = self.processor.apply_chat_template(\n",
    "            conversation, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        inputs = self.processor(\n",
    "            text=text,\n",
    "            images=image,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        \n",
    "        user_tokenized = self.processor.tokenizer.apply_chat_template(\n",
    "            [conversation[0]],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        user_length = user_tokenized.shape[1]\n",
    "        labels[:, :user_length] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"labels\": labels.squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "try:\n",
    "    train_dataset = COCODataset(dataset_stream, processor, max_samples=config.max_samples)\n",
    "    print(f\"COCO dataset ready with {len(train_dataset)} samples for fine-tuning.\")\n",
    "    \n",
    "   \n",
    "    if len(train_dataset) > 0:\n",
    "        test_sample = train_dataset[0]\n",
    "        print(f\"Sample shapes:\")\n",
    "        for key, value in test_sample.items():\n",
    "            print(f\"  {key}: {value.shape}\")\n",
    "    else:\n",
    "        print(\"Warning: Dataset is empty!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset: {e}\")\n",
    "    print(\"Please check the dataset format and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1f7d4-dbc9-4267-95af-9442780fb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA...\n",
      "Trainable parameters: 4,980,736\n",
      "Total parameters: 7,068,407,808\n",
      "Percentage trainable: 0.07%\n",
      "LoRA applied successfully and model ready for training!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Configure and apply LoRA (Low-Rank Adaptation) safely.\n",
    "Ensures modules exist, supports k-bit training, and prepares for checkpoints.\n",
    "\"\"\"\n",
    "print(\"Configuring LoRA...\")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "config.lora_r = 8               \n",
    "config.lora_alpha = 16          \n",
    "config.lora_dropout = 0.05      \n",
    "config.lora_target_modules = [\"q_proj\", \"v_proj\"]  \n",
    "\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    target_modules=config.lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "\n",
    "valid_modules = [name for name, _ in model.named_modules()]\n",
    "missing = [m for m in lora_config.target_modules if not any(m in v for v in valid_modules)]\n",
    "if missing:\n",
    "    print(f\"Warning: Some LoRA target_modules not found in model: {missing}\")\n",
    "    print(f\"Available modules (sample): {valid_modules[:50]}\")\n",
    "\n",
    "    lora_config.target_modules = [m for m in lora_config.target_modules if m in valid_modules]\n",
    "\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "print(\"LoRA applied successfully and model ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ac3de-a2c0-49df-a4c5-d9eefe03ec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "No valid cached dataset found. Processing datasets from scratch...\n",
      "Datasets processed and cached successfully!\n",
      "Training samples: 74505\n",
      "Validation samples: 8278\n",
      "Data collator created successfully!\n",
      "Using placeholder image for sample 81069 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000397355.jpg)\n",
      "Using placeholder image for sample 81069 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000397355.jpg)\n",
      "Data collator test successful. Batch shapes:\n",
      "  input_ids: torch.Size([2, 10])\n",
      "  attention_mask: torch.Size([2, 10])\n",
      "  pixel_values: torch.Size([2, 3, 336, 336])\n",
      "  labels: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Create or load cached training/validation datasets and define the data collator.\n",
    "This avoids reprocessing the dataset on restart and ensures reproducibility.\n",
    "\"\"\"\n",
    "import pickle\n",
    "import json\n",
    "print(\"Preparing datasets...\")\n",
    "\n",
    "cache_dir = os.path.join(config.output_dir, \"dataset_cache\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "train_cache_path = os.path.join(cache_dir, \"train_dataset.pkl\")\n",
    "val_cache_path = os.path.join(cache_dir, \"val_dataset.pkl\")\n",
    "cache_info_path = os.path.join(cache_dir, \"cache_info.json\")\n",
    "\n",
    "\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "    print(f\"pad_token_id was None, set to eos_token_id ({processor.tokenizer.eos_token_id})\")\n",
    "\n",
    "\n",
    "cache_valid = False\n",
    "if os.path.exists(train_cache_path) and os.path.exists(val_cache_path) and os.path.exists(cache_info_path):\n",
    "    try:\n",
    "        with open(cache_info_path, \"r\") as f:\n",
    "            cache_info = json.load(f)\n",
    "\n",
    "        if (cache_info.get(\"dataset_name\") == config.dataset_name and \n",
    "            cache_info.get(\"max_samples\") == config.max_samples):\n",
    "            cache_valid = True\n",
    "            print(\"Found valid cached datasets...\")\n",
    "    except:\n",
    "        print(\"Cache info corrupted, will regenerate...\")\n",
    "\n",
    "if cache_valid:\n",
    "    print(\"Loading cached datasets...\")\n",
    "    with open(train_cache_path, \"rb\") as f:\n",
    "        train_dataset = pickle.load(f)\n",
    "    with open(val_cache_path, \"rb\") as f:\n",
    "        val_dataset = pickle.load(f)\n",
    "else:\n",
    "    print(\"No valid cached dataset found. Processing datasets from scratch...\")\n",
    "    \n",
    "\n",
    "    dataset_stream = load_dataset(config.dataset_name, split=\"train\", streaming=True)\n",
    "    \n",
    "  \n",
    "    full_dataset = COCODataset(\n",
    "        dataset_stream, \n",
    "        processor, \n",
    "        max_length=512, \n",
    "        max_samples=config.max_samples\n",
    "    )\n",
    "    \n",
    "  \n",
    "    dataset_size = len(full_dataset)\n",
    "    val_size = max(1, int(0.1 * dataset_size))  \n",
    "    train_size = dataset_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "\n",
    "    try:\n",
    "        with open(train_cache_path, \"wb\") as f:\n",
    "            pickle.dump(train_dataset, f)\n",
    "        with open(val_cache_path, \"wb\") as f:\n",
    "            pickle.dump(val_dataset, f)\n",
    "        \n",
    "\n",
    "        cache_info = {\n",
    "            \"dataset_name\": config.dataset_name,\n",
    "            \"max_samples\": config.max_samples,\n",
    "            \"train_size\": train_size,\n",
    "            \"val_size\": val_size\n",
    "        }\n",
    "        with open(cache_info_path, \"w\") as f:\n",
    "            json.dump(cache_info, f)\n",
    "            \n",
    "        print(\"Datasets processed and cached successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not cache datasets: {e}\")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "class LLaVADataCollator:\n",
    "    \"\"\"\n",
    "    Custom data collator for LLaVA training.\n",
    "    Handles proper batching of images, text, and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "  \n",
    "        input_ids = [item[\"input_ids\"] for item in batch]\n",
    "        attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "        pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "        labels = [item[\"labels\"] for item in batch]\n",
    "        \n",
    " \n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        attention_masks = torch.nn.utils.rnn.pad_sequence(\n",
    "            attention_masks, batch_first=True, padding_value=0\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=-100\n",
    "        )\n",
    "        \n",
    "      \n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "data_collator = LLaVADataCollator(processor)\n",
    "print(\"Data collator created successfully!\")\n",
    "\n",
    "\n",
    "if len(train_dataset) > 0:\n",
    "    try:\n",
    "        sample_batch = [train_dataset[0], train_dataset[0]]\n",
    "        test_batch = data_collator(sample_batch)\n",
    "        print(f\"Data collator test successful. Batch shapes:\")\n",
    "        for key, value in test_batch.items():\n",
    "            print(f\"  {key}: {value.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Data collator test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886345c1-72b0-4b33-9a30-1ac0ed61bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.56.1\n",
      "SentencePiece version: 0.2.1\n",
      "PyTorch version: 2.8.0.dev20250319+cu128\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "bfloat16 supported: True\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Cell: Check Transformers & SentencePiece Versions\n",
    "# ------------------------------\n",
    "import transformers\n",
    "import sentencepiece\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"SentencePiece version: {sentencepiece.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "print(f\"bfloat16 supported: {torch.cuda.is_bf16_supported()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779d3dc-1140-48e4-b111-6a020d863ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training arguments...\n",
      "✓ TrainingArguments created successfully (Single-GPU, FP16)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Training Arguments Configuration (Restart-Safe, FP16, No DeepSpeed)\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "print(\"Setting up training arguments...\")\n",
    "\n",
    "\n",
    "defaults = {\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"logging_steps\": 10,\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 50,\n",
    "    \"output_dir\": \"./output\"\n",
    "}\n",
    "\n",
    "\n",
    "for key, value in defaults.items():\n",
    "    if not hasattr(config, key):\n",
    "        setattr(config, key, value)\n",
    "        print(f\"Set default {key}: {value}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    max_grad_norm=config.max_grad_norm,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{config.output_dir}/logs\",\n",
    "    logging_steps=config.logging_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config.save_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=config.dataloader_num_workers,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,   \n",
    "    bf16=False,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "print(\"✓ TrainingArguments created successfully (Single-GPU, FP16)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f951f-bdc1-4b80-96ba-72d3e8123b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fixed LLaVA dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033401df5fdc477fb31c3eaa8e3cf077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1f1db1f5da49dc9ed627eefa9156f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COCO.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for HuggingFaceM4/COCO contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/COCO.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b43b51c9fa4322b8adf1b8167754a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/36.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103debe5cfbf488cb378ef96fba1f1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FSTimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/aiohttp/streams.py:347\u001b[39m, in \u001b[36mStreamReader._wait\u001b[39m\u001b[34m(self, func_name)\u001b[39m\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._timer:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py:56\u001b[39m, in \u001b[36m_runner\u001b[39m\u001b[34m(event, coro, result, timeout)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     result[\u001b[32m0\u001b[39m] = \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/http.py:262\u001b[39m, in \u001b[36mHTTPFileSystem._get_file\u001b[39m\u001b[34m(self, rpath, lpath, chunk_size, callback, **kwargs)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m chunk:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     chunk = \u001b[38;5;28;01mawait\u001b[39;00m r.content.read(chunk_size)\n\u001b[32m    263\u001b[39m     outfile.write(chunk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/aiohttp/streams.py:428\u001b[39m, in \u001b[36mStreamReader.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_nowait(n)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/aiohttp/streams.py:346\u001b[39m, in \u001b[36mStreamReader._wait\u001b[39m\u001b[34m(self, func_name)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwaiter\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/aiohttp/helpers.py:685\u001b[39m, in \u001b[36mTimerContext.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    684\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m asyncio.TimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_val\u001b[39;00m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFSTimeoutError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# Load a small sample of COCO dataset for testing\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFaceM4/COCO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m small_dataset = dataset.select(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[32m100\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset))))  \u001b[38;5;66;03m# Take first 100 samples\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Create LLaVA dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/load.py:2084\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2081\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   2083\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2084\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2093\u001b[39m keep_in_memory = (\n\u001b[32m   2094\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2095\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py:925\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    924\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py:1649\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[39m\n\u001b[32m   1648\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, **prepare_splits_kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1649\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py:979\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    977\u001b[39m split_dict = SplitDict(dataset_name=\u001b[38;5;28mself\u001b[39m.dataset_name)\n\u001b[32m    978\u001b[39m split_generators_kwargs = \u001b[38;5;28mself\u001b[39m._make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m split_generators = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verification_mode == VerificationMode.ALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager.record_checksums:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/datasets_modules/datasets/HuggingFaceM4--COCO/7dba4b1d9b12e588770263b87c2a1fd7b03072c0a0cf550c896dbd09bf2bb7f8/COCO.py:130\u001b[39m, in \u001b[36mCOCO._split_generators\u001b[39m\u001b[34m(self, dl_manager)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[32m    129\u001b[39m     annotation_file = os.path.join(dl_manager.download_and_extract(_KARPATHY_FILES_URL), \u001b[33m\"\u001b[39m\u001b[33mdataset_coco.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     image_folders = {k: Path(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdl_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_IMAGES_URLS\u001b[49m\u001b[43m)\u001b[49m.items()}\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    133\u001b[39m         datasets.SplitGenerator(\n\u001b[32m    134\u001b[39m             name=datasets.Split.TRAIN,\n\u001b[32m   (...)\u001b[39m\u001b[32m    156\u001b[39m         ),\n\u001b[32m    157\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py:326\u001b[39m, in \u001b[36mDownloadManager.download_and_extract\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[32m    311\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[32m    312\u001b[39m \n\u001b[32m    313\u001b[39m \u001b[33;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.extract(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py:159\u001b[39m, in \u001b[36mDownloadManager.download\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    157\u001b[39m start_time = datetime.now()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     downloaded_path_or_paths = \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDownloading data files\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m duration = datetime.now() - start_time\n\u001b[32m    169\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration.total_seconds()\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py:521\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    519\u001b[39m         batch_size = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) // num_proc + \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) % num_proc > \u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m)\n\u001b[32m    520\u001b[39m     iterable = \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m mapped = \u001b[43m[\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    526\u001b[39m     mapped = [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py:522\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    519\u001b[39m         batch_size = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) // num_proc + \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) % num_proc > \u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m)\n\u001b[32m    520\u001b[39m     iterable = \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[32m    521\u001b[39m mapped = [\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\n\u001b[32m    524\u001b[39m ]\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    526\u001b[39m     mapped = [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py:390\u001b[39m, in \u001b[36m_single_map_nested\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    383\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    385\u001b[39m     batched\n\u001b[32m    386\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    387\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[32m    388\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[32m    389\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mmapped_item\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapped_item\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging.get_verbosity() < logging.WARNING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py:390\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    383\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    385\u001b[39m     batched\n\u001b[32m    386\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    387\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[32m    388\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[32m    389\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging.get_verbosity() < logging.WARNING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py:219\u001b[39m, in \u001b[36mDownloadManager._download_batched\u001b[39m\u001b[34m(self, url_or_filenames, download_config)\u001b[39m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[32m    207\u001b[39m         download_func,\n\u001b[32m    208\u001b[39m         url_or_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m         tqdm_class=tqdm,\n\u001b[32m    217\u001b[39m     )\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl_or_filenames\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py:220\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[32m    207\u001b[39m         download_func,\n\u001b[32m    208\u001b[39m         url_or_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m         tqdm_class=tqdm,\n\u001b[32m    217\u001b[39m     )\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[32m    222\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py:229\u001b[39m, in \u001b[36mDownloadManager._download_single\u001b[39m\u001b[34m(self, url_or_filename, download_config)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[32m    228\u001b[39m     url_or_filename = url_or_path_join(\u001b[38;5;28mself\u001b[39m._base_path, url_or_filename)\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m out = \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m out = tracked_str(out)\n\u001b[32m    231\u001b[39m out.set_origin(url_or_filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/utils/file_utils.py:214\u001b[39m, in \u001b[36mcached_path\u001b[39m\u001b[34m(url_or_filename, download_config, **download_kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    212\u001b[39m     \u001b[38;5;66;03m# Download external files\u001b[39;00m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m         output_path = \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_etag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse_etag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdownload_desc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m os.path.exists(url_or_filename):\n\u001b[32m    226\u001b[39m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[32m    227\u001b[39m     output_path = url_or_filename\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/utils/file_utils.py:415\u001b[39m, in \u001b[36mget_from_cache\u001b[39m\u001b[34m(url, cache_dir, force_download, user_agent, use_etag, token, storage_options, download_desc, disable_tqdm)\u001b[39m\n\u001b[32m    413\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    414\u001b[39m     \u001b[38;5;66;03m# GET file object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[43mfsspec_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    418\u001b[39m shutil.move(temp_file.name, cache_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/utils/file_utils.py:339\u001b[39m, in \u001b[36mfsspec_get\u001b[39m\u001b[34m(url, temp_file, storage_options, desc, disable_tqdm)\u001b[39m\n\u001b[32m    326\u001b[39m fs, path = url_to_fs(url, **(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[32m    327\u001b[39m callback = TqdmCallback(\n\u001b[32m    328\u001b[39m     tqdm_kwargs={\n\u001b[32m    329\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdesc\u001b[39m\u001b[33m\"\u001b[39m: desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mDownloading\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    337\u001b[39m     }\n\u001b[32m    338\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m \u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py:118\u001b[39m, in \u001b[36msync_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mself\u001b[39m = obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py:101\u001b[39m, in \u001b[36msync\u001b[39m\u001b[34m(loop, func, timeout, *args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m return_result = result[\u001b[32m0\u001b[39m]\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, asyncio.TimeoutError):\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# suppress asyncio.TimeoutError, raise FSTimeoutError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreturn_result\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n",
      "\u001b[31mFSTimeoutError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Create proper dataset and data collator that handles LLaVA's <image> token requirements.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class LLaVADataset(Dataset):\n",
    "    \"\"\"Custom dataset that properly formats inputs for LLaVA training.\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset, processor, image_token=\"<image>\", max_length=512):\n",
    "        self.dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "        self.image_token = image_token\n",
    "        self.max_length = max_length\n",
    "        \n",
    "\n",
    "        if self.image_token not in self.processor.tokenizer.vocab:\n",
    "            self.processor.tokenizer.add_tokens([self.image_token])\n",
    "            print(f\"Added {self.image_token} token to tokenizer\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "\n",
    "        if 'image' in item:\n",
    "            image = item['image']\n",
    "            if isinstance(image, str):\n",
    "\n",
    "                response = requests.get(image)\n",
    "                image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            elif not isinstance(image, Image.Image):\n",
    "                image = Image.fromarray(image).convert('RGB')\n",
    "        else:\n",
    "\n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "        \n",
    "\n",
    "        if 'caption' in item:\n",
    "            caption = item['caption']\n",
    "        elif 'text' in item:\n",
    "            caption = item['text']\n",
    "        else:\n",
    "            caption = \"Describe this image.\"\n",
    "        \n",
    "\n",
    "        conversation_text = f\"Human: {self.image_token} {caption}\\nAssistant: This is an image showing {caption}\"\n",
    "        \n",
    "\n",
    "        image_inputs = self.processor.image_processor(image, return_tensors=\"pt\")\n",
    "        \n",
    "\n",
    "        text_inputs = self.processor.tokenizer(\n",
    "            conversation_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        labels = text_inputs[\"input_ids\"].clone()\n",
    "        \n",
    "\n",
    "        assistant_start = conversation_text.find(\"Assistant:\")\n",
    "        if assistant_start != -1:\n",
    "\n",
    "            assistant_tokens = self.processor.tokenizer(\n",
    "                conversation_text[:assistant_start], \n",
    "                return_tensors=\"pt\"\n",
    "            )[\"input_ids\"]\n",
    "            labels[:, :assistant_tokens.shape[1]] = -100\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image_inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"input_ids\": text_inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": text_inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels.squeeze(0)\n",
    "        }\n",
    "\n",
    "class LLaVADataCollator:\n",
    "    \"\"\"Data collator that properly handles LLaVA multimodal inputs.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, pad_token_id=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = pad_token_id or tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "\n",
    "        pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "        \n",
    "\n",
    "        max_length = max(item[\"input_ids\"].shape[0] for item in batch)\n",
    "        \n",
    "        # Pad input_ids, attention_mask, and labels\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        \n",
    "        for item in batch:\n",
    "            seq_len = item[\"input_ids\"].shape[0]\n",
    "            pad_length = max_length - seq_len\n",
    "            \n",
    "\n",
    "            padded_input_ids = torch.cat([\n",
    "                item[\"input_ids\"], \n",
    "                torch.full((pad_length,), self.pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "            input_ids.append(padded_input_ids)\n",
    "            \n",
    "\n",
    "            padded_attention = torch.cat([\n",
    "                item[\"attention_mask\"],\n",
    "                torch.zeros(pad_length, dtype=torch.long)\n",
    "            ])\n",
    "            attention_mask.append(padded_attention)\n",
    "            \n",
    "\n",
    "            padded_labels = torch.cat([\n",
    "                item[\"labels\"],\n",
    "                torch.full((pad_length,), -100, dtype=torch.long)\n",
    "            ])\n",
    "            labels.append(padded_labels)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": torch.stack(input_ids),\n",
    "            \"attention_mask\": torch.stack(attention_mask),\n",
    "            \"labels\": torch.stack(labels)\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Creating fixed LLaVA dataset...\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"HuggingFaceM4/COCO\", split=\"train\", streaming=False)\n",
    "small_dataset = dataset.select(range(min(100, len(dataset))))  \n",
    "\n",
    "train_dataset = LLaVADataset(\n",
    "    hf_dataset=small_dataset,\n",
    "    processor=processor,\n",
    "    image_token=\"<image>\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "data_collator = LLaVADataCollator(\n",
    "    tokenizer=processor.tokenizer,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(train_dataset)} samples\")\n",
    "\n",
    "\n",
    "print(\"Testing dataset...\")\n",
    "try:\n",
    "    sample = train_dataset[0]\n",
    "    print(\"Sample keys:\", sample.keys())\n",
    "    print(\"Input IDs shape:\", sample[\"input_ids\"].shape)\n",
    "    print(\"Pixel values shape:\", sample[\"pixel_values\"].shape)\n",
    "    print(\"Labels shape:\", sample[\"labels\"].shape)\n",
    "    \n",
    "  \n",
    "    decoded_text = processor.tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=False)\n",
    "    has_image_token = \"<image>\" in decoded_text\n",
    "    print(f\"Text contains <image> token: {has_image_token}\")\n",
    "    print(f\"Sample text: {decoded_text[:200]}...\")\n",
    "    \n",
    "    \n",
    "    batch = data_collator([sample])\n",
    "    print(\"Batch keys:\", batch.keys())\n",
    "    print(\"Batch pixel_values shape:\", batch[\"pixel_values\"].shape)\n",
    "    print(\"Batch input_ids shape:\", batch[\"input_ids\"].shape)\n",
    "    \n",
    "    if has_image_token:\n",
    "        print(\"✓ Dataset properly formatted for LLaVA!\")\n",
    "    else:\n",
    "        print(\"⚠ Warning: <image> token not found in text\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Dataset test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Dataset and data collator ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85423835-01c9-4985-a3be-d15dcc9d26b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data collator ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Custom collator for image + text inputs.\n",
    "Handles batching of pixel_values and tokenized inputs for LoRA fine-tuning.\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class LLaVADataCollator:\n",
    "    def __init__(self, tokenizer, image_key=\"pixel_values\", text_keys=[\"input_ids\", \"attention_mask\", \"labels\"]):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_key = image_key\n",
    "        self.text_keys = text_keys\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        collated = {}\n",
    "\n",
    "\n",
    "        collated[self.image_key] = torch.stack([b[self.image_key] for b in batch])\n",
    "\n",
    "\n",
    "        for key in self.text_keys:\n",
    "            collated[key] = nn.utils.rnn.pad_sequence(\n",
    "                [b[key].squeeze(0) for b in batch],\n",
    "                batch_first=True,\n",
    "                padding_value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        return collated\n",
    "\n",
    "# Create instance\n",
    "data_collator = LLaVADataCollator(tokenizer=processor.tokenizer)\n",
    "print(\"✅ Data collator ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d348b5-8aff-44c8-8c0e-13f53bd34c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing standard Hugging Face trainer...\n",
      "Created split - Training: 59603, Validation: 14900\n",
      "Clean training arguments created without DeepSpeed\n",
      "Batch size: 2\n",
      "Gradient accumulation: 8\n",
      "Effective batch size: 16\n",
      "Learning rate: 0.0002\n",
      "Evaluation enabled: True\n",
      "Standard Hugging Face Trainer initialized successfully!\n",
      "Model type: LlavaForConditionalGeneration\n",
      "Trainer type: LLaVATrainerCustom\n",
      "No existing checkpoints found.\n",
      "\n",
      "Training setup verified:\n",
      "- Training samples: 59603\n",
      "- Validation samples: 14900\n",
      "- Output directory: ./llava-coco-lora\n",
      "\n",
      "GPU Status:\n",
      "- Available devices: 1\n",
      "- Current device: 0\n",
      "- Total memory: 85.1GB\n",
      "- Allocated memory: 28.3GB\n",
      "- Free memory: 56.8GB\n",
      "\n",
      "==================================================\n",
      "STARTING TRAINING\n",
      "==================================================\n",
      "Using placeholder image for sample 23355 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000574217.jpg)\n",
      "Using placeholder image for sample 68015 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000539296.jpg)\n",
      "Using placeholder image for sample 9866 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000324894.jpg)\n",
      "Using placeholder image for sample 15383 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000568113.jpg)\n",
      "Using placeholder image for sample 67717 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000076276.jpg)\n",
      "Using placeholder image for sample 28169 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000425721.jpg)\n",
      "Using placeholder image for sample 51487 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000174890.jpg)\n",
      "Using placeholder image for sample 40839 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000365142.jpg)\n",
      "Using placeholder image for sample 68931 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000402729.jpg)\n",
      "Using placeholder image for sample 71195 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000495905.jpg)\n",
      "Using placeholder image for sample 7464 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000134286.jpg)\n",
      "Using placeholder image for sample 80158 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000267229.jpg)\n",
      "Using placeholder image for sample 60395 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000557266.jpg)\n",
      "Using placeholder image for sample 64694 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000312861.jpg)\n",
      "Using placeholder image for sample 66564 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000511031.jpg)\n",
      "Using placeholder image for sample 56422 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000127100.jpg)\n",
      "Using placeholder image for sample 27407 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000003511.jpg)\n",
      "Using placeholder image for sample 19965 (URL: http://images.cocodataset.org/train2014/COCO_train2014_000000082779.jpg)\n",
      "\n",
      "Training failed: ValueError: Image features and image tokens do not match: tokens: 0, features 4718592\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image features and image tokens do not match: tokens: 0, features 4718592",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 157\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Extract and save metrics\u001b[39;00m\n\u001b[32m    160\u001b[39m metrics = train_result.metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:4009\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4008\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4009\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4011\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4013\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4014\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4015\u001b[39m ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mLLaVATrainerCustom.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m labels = inputs.get(\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Use the model's built-in loss calculation\u001b[39;00m\n\u001b[32m     79\u001b[39m     loss = outputs.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/llava/modeling_llava.py:445\u001b[39m, in \u001b[36mLlavaForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, image_sizes, **kwargs)\u001b[39m\n\u001b[32m    436\u001b[39m vision_feature_layer = (\n\u001b[32m    437\u001b[39m     vision_feature_layer \u001b[38;5;28;01mif\u001b[39;00m vision_feature_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.vision_feature_layer\n\u001b[32m    438\u001b[39m )\n\u001b[32m    439\u001b[39m vision_feature_select_strategy = (\n\u001b[32m    440\u001b[39m     vision_feature_select_strategy\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m vision_feature_select_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.vision_feature_select_strategy\n\u001b[32m    443\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_feature_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_feature_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/llava/modeling_llava.py:293\u001b[39m, in \u001b[36mLlavaModel.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, image_sizes, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m     image_features = \u001b[38;5;28mself\u001b[39m.get_image_features(\n\u001b[32m    287\u001b[39m         pixel_values=pixel_values,\n\u001b[32m    288\u001b[39m         vision_feature_layer=vision_feature_layer,\n\u001b[32m    289\u001b[39m         vision_feature_select_strategy=vision_feature_select_strategy,\n\u001b[32m    290\u001b[39m         image_sizes=image_sizes,\n\u001b[32m    291\u001b[39m     )\n\u001b[32m    292\u001b[39m     image_features = torch.cat(image_features, dim=\u001b[32m0\u001b[39m).to(inputs_embeds.device, inputs_embeds.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     special_image_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_placeholder_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_features\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n\u001b[32m    298\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.language_model(\n\u001b[32m    299\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    300\u001b[39m     position_ids=position_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    308\u001b[39m     **kwargs,\n\u001b[32m    309\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/llava/modeling_llava.py:240\u001b[39m, in \u001b[36mLlavaModel.get_placeholder_mask\u001b[39m\u001b[34m(self, input_ids, inputs_embeds, image_features)\u001b[39m\n\u001b[32m    238\u001b[39m n_image_features = image_features.shape[\u001b[32m0\u001b[39m] * image_features.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds[special_image_mask].numel() != image_features.numel():\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    241\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage features and image tokens do not match: tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_image_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, features \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_image_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m     )\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m special_image_mask\n",
      "\u001b[31mValueError\u001b[39m: Image features and image tokens do not match: tokens: 0, features 4718592"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Initialize standard Hugging Face Trainer for single-GPU LLaVA fine-tuning.\n",
    "This avoids DeepSpeed and custom trainer classes that can cause conflicts.\n",
    "\"\"\"\n",
    "print(\"Initializing standard Hugging Face trainer...\")\n",
    "from torch.utils.data import random_split\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "import os\n",
    "\n",
    "\n",
    "min_val_size = 2\n",
    "val_size = max(min_val_size, len(train_dataset) // 5)\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "\n",
    "if val_size >= min_val_size and len(train_dataset) > 10:\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    use_evaluation = True\n",
    "    print(f\"Created split - Training: {len(train_dataset)}, Validation: {len(val_dataset)}\")\n",
    "else:\n",
    "    val_dataset = None\n",
    "    use_evaluation = False\n",
    "    print(f\"Dataset too small for validation. Training on {len(train_dataset)} samples only.\")\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args_clean = TrainingArguments(\n",
    "    output_dir=getattr(config, 'output_dir', './output'),\n",
    "    num_train_epochs=getattr(config, 'num_train_epochs', 3),\n",
    "    per_device_train_batch_size=getattr(config, 'per_device_train_batch_size', 1),\n",
    "    per_device_eval_batch_size=getattr(config, 'per_device_eval_batch_size', 1),\n",
    "    gradient_accumulation_steps=getattr(config, 'gradient_accumulation_steps', 4),\n",
    "    learning_rate=getattr(config, 'learning_rate', 2e-4),\n",
    "    warmup_steps=getattr(config, 'warmup_steps', 100),\n",
    "    max_grad_norm=getattr(config, 'max_grad_norm', 1.0),\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{getattr(config, 'output_dir', './output')}/logs\",\n",
    "    logging_steps=getattr(config, 'logging_steps', 10),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=getattr(config, 'save_steps', 100),\n",
    "    eval_strategy=\"steps\" if use_evaluation else \"no\",\n",
    "    eval_steps=getattr(config, 'eval_steps', 100) if use_evaluation else None,\n",
    "    load_best_model_at_end=use_evaluation,\n",
    "    metric_for_best_model=\"eval_loss\" if use_evaluation else None,\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "    remove_unused_columns=False,  # Keep all columns for multimodal data\n",
    "    fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    report_to=None,  \n",
    ")\n",
    "\n",
    "print(\"Clean training arguments created without DeepSpeed\")\n",
    "print(f\"Batch size: {training_args_clean.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {training_args_clean.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args_clean.per_device_train_batch_size * training_args_clean.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args_clean.learning_rate}\")\n",
    "print(f\"Evaluation enabled: {use_evaluation}\")\n",
    "# --- Create Custom Trainer Class for LLaVA ---\n",
    "class LLaVATrainerCustom(Trainer):\n",
    "    \"\"\"Custom trainer for LLaVA that properly handles multimodal inputs.\"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Custom loss computation for LLaVA multimodal training.\"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = outputs.loss\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = inputs[\"input_ids\"][..., 1:].contiguous()\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = LLaVATrainerCustom(\n",
    "    model=model,\n",
    "    args=training_args_clean,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset if use_evaluation else None,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=3, \n",
    "            early_stopping_threshold=0.01\n",
    "        )\n",
    "    ] if use_evaluation else []\n",
    ")\n",
    "\n",
    "print(\"Standard Hugging Face Trainer initialized successfully!\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Trainer type: {type(trainer).__name__}\")\n",
    "\n",
    "checkpoint_dir = None\n",
    "output_dir = training_args_clean.output_dir\n",
    "if os.path.exists(output_dir):\n",
    "    checkpoints = [\n",
    "        os.path.join(output_dir, d) for d in os.listdir(output_dir)\n",
    "        if os.path.isdir(os.path.join(output_dir, d)) and d.startswith(\"checkpoint\")\n",
    "    ]\n",
    "    if checkpoints:\n",
    "        checkpoint_dir = max(checkpoints, key=os.path.getmtime)\n",
    "        print(f\"Found checkpoint: {checkpoint_dir}\")\n",
    "    else:\n",
    "        print(\"No existing checkpoints found.\")\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    raise ValueError(\"Train dataset is empty!\")\n",
    "\n",
    "print(f\"\\nTraining setup verified:\")\n",
    "print(f\"- Training samples: {len(train_dataset)}\")\n",
    "print(f\"- Validation samples: {len(val_dataset) if val_dataset else 0}\")\n",
    "print(f\"- Output directory: {output_dir}\")\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        device_count = torch.cuda.device_count()\n",
    "        current_device = torch.cuda.current_device()\n",
    "        total_memory = torch.cuda.get_device_properties(current_device).total_memory / 1e9\n",
    "        allocated_memory = torch.cuda.memory_allocated(current_device) / 1e9\n",
    "        \n",
    "        print(f\"\\nGPU Status:\")\n",
    "        print(f\"- Available devices: {device_count}\")\n",
    "        print(f\"- Current device: {current_device}\")\n",
    "        print(f\"- Total memory: {total_memory:.1f}GB\")\n",
    "        print(f\"- Allocated memory: {allocated_memory:.1f}GB\")\n",
    "        print(f\"- Free memory: {total_memory - allocated_memory:.1f}GB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint_dir)\n",
    "    \n",
    "    metrics = train_result.metrics\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Final training loss: {metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "    print(f\"Training runtime: {metrics.get('train_runtime', 'N/A'):.2f} seconds\")\n",
    "    print(f\"Training samples per second: {metrics.get('train_samples_per_second', 'N/A'):.2f}\")\n",
    "    \n",
    "    if use_evaluation and 'eval_loss' in metrics:\n",
    "        print(f\"Final validation loss: {metrics.get('eval_loss', 'N/A'):.4f}\")\n",
    "    \n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(output_dir) \n",
    "    \n",
    "    print(f\"\\nModel and processor saved to: {output_dir}\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    error_str = str(e)\n",
    "    print(f\"\\nRuntimeError occurred: {error_str}\")\n",
    "    \n",
    "    if \"CUDA out of memory\" in error_str:\n",
    "        print(\"\\nCUDA OUT OF MEMORY SOLUTIONS:\")\n",
    "        print(\"1. Reduce per_device_train_batch_size (current: {})\".format(training_args_clean.per_device_train_batch_size))\n",
    "        print(\"2. Increase gradient_accumulation_steps (current: {})\".format(training_args_clean.gradient_accumulation_steps))\n",
    "        print(\"3. Enable gradient checkpointing in model\")\n",
    "        print(\"4. Use a smaller model or reduce max_length\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "            reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "            print(f\"\\nGPU Memory at error:\")\n",
    "            print(f\"- Allocated: {allocated:.1f}GB\")\n",
    "            print(f\"- Reserved: {reserved:.1f}GB\")\n",
    "    raise\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining failed: {type(e).__name__}: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nTraining process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7acef-4dc7-4010-b9c1-c021992eca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the fine-tuned LLaVA model, LoRA adapters, and processor.\n",
    "Ensures restart-safe reloads without DeepSpeed.\n",
    "\"\"\"\n",
    "print(\"Saving fine-tuned model...\")\n",
    "\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "trainer.save_model(config.output_dir)\n",
    "\n",
    "processor.save_pretrained(config.output_dir)\n",
    "\n",
    "adapter_dir = os.path.join(config.output_dir, \"lora_adapter\")\n",
    "os.makedirs(adapter_dir, exist_ok=True)\n",
    "model.save_pretrained(adapter_dir)\n",
    "\n",
    "print(f\"✅ Model and LoRA adapter saved under {config.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce855b-be60-4f96-888e-90e02d53af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate the fine-tuned model and test on a sample image.\n",
    "Safe token slicing and device handling.\n",
    "\"\"\"\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "if val_dataset is not None and len(val_dataset) > 0:\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Evaluation Results:\")\n",
    "    for key, value in eval_results.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"No validation dataset provided, skipping evaluation.\")\n",
    "\n",
    "if val_dataset is not None and len(val_dataset) > 0:\n",
    "    print(\"\\nTesting model on a sample image...\")\n",
    "    sample_idx = 0\n",
    "    sample = val_dataset[sample_idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(model.device)\n",
    "        input_text = f\"{config.image_token}\\nDescribe this image.\"\n",
    "\n",
    "        inputs = processor(\n",
    "            text=input_text,\n",
    "            images=None,  \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        inputs = {k: v.to(model.device) if torch.is_tensor(v) else v for k, v in inputs.items()}\n",
    "        inputs[\"pixel_values\"] = pixel_values\n",
    "\n",
    "       \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "       \n",
    "        generated_text = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        input_prompt = processor.tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "        generated_response = generated_text[len(input_prompt):].strip()\n",
    "\n",
    "        print(f\"Generated description: {generated_response}\")\n",
    "else:\n",
    "    print(\"No validation samples available for testing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ad1b8-cdd6-4065-82ae-fd0ac50416ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINE-TUNING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Model: {config.model_name}\")\n",
    "print(f\"✓ Dataset: {getattr(config, 'dataset_name', 'custom')} ({getattr(config, 'max_samples', len(train_dataset))} samples)\")\n",
    "print(f\"✓ Training method: LoRA (no DeepSpeed)\")\n",
    "if 'trainable_params' in globals() and 'total_params' in globals():\n",
    "    print(f\"✓ Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2*_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db86223-cacb-4af8-9f80-5bdeff4b4d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
