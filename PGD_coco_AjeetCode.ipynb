{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97693649-e105-40e3-9c3c-e4eaa6408feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Core Hugging Face library\n",
    "!pip install transformers\n",
    "\n",
    "# LLaVA checkpoints are hosted on the hub, so you need this too\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d03e08-7393-484d-a032-99c849542ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/workspace/hf_cache\"\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "513484f6-508b-425a-aad7-d8eb3fc7b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_subseq_start(big_ids, small_ids):\n",
    "    B, S = big_ids.tolist(), small_ids.tolist()\n",
    "    for i in range(0, len(B) - len(S) + 1):\n",
    "        if B[i:i+len(S)] == S:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "@torch.no_grad()\n",
    "def _generate_caption(model, processor, image_pil, prompt, device):\n",
    "    inputs = processor(text=prompt, images=[image_pil], return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    out_ids = model.generate(**inputs, max_new_tokens=120)\n",
    "    txt = processor.batch_decode(out_ids, skip_special_tokens=True)[0].strip()\n",
    "    # Best-effort prompt echo strip (optional)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28b1630-6611-43ac-8156-2055cbededc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PGD attack (with sanity checks + .to fix) ---\n",
    "def pgd_attack_image_llava(\n",
    "    model,\n",
    "    processor,\n",
    "    image_pil,\n",
    "    prompt = \"<image>\\nDescribe this image in detail.\",\n",
    "    epsilon = 8/255,       # L∞ pixel-space\n",
    "    alpha   = 1/255,       # step size in pixel-space\n",
    "    num_steps = 40,\n",
    "    device = \"cuda\",\n",
    "    random_start = True,\n",
    "    verbose = True\n",
    "):\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce mem during loss/backprop\n",
    "\n",
    "    # 1) Clean caption to derive the answer tokens we’ll attack\n",
    "    clean_caption = _generate_caption(model, processor, image_pil, prompt, device)\n",
    "\n",
    "    # 2) Pack prompt + answer as the training text\n",
    "    attack_text = prompt + \" \" + clean_caption\n",
    "    packed = processor(text=attack_text, images=[image_pil], return_tensors=\"pt\")\n",
    "\n",
    "    # Get normalization stats\n",
    "    image_processor = processor.image_processor\n",
    "    mean = torch.tensor(image_processor.image_mean, device=device, dtype=torch.float32).view(1,3,1,1)\n",
    "    std  = torch.tensor(image_processor.image_std,  device=device, dtype=torch.float32).view(1,3,1,1)\n",
    "\n",
    "    # Convert normalized pixel_values -> pixel space [0,1] with proper .to usage\n",
    "    with torch.no_grad():\n",
    "        norm_px = packed[\"pixel_values\"]  # float32 normalized\n",
    "        # to() fix: specify both device and dtype as keywords\n",
    "        norm_px = norm_px.to(device=device, dtype=model.dtype)\n",
    "        mean_   = mean.to(device=device, dtype=model.dtype)\n",
    "        std_    = std.to(device=device, dtype=model.dtype)\n",
    "        orig_px = (norm_px * std_ + mean_).clamp(0, 1)\n",
    "\n",
    "    # Text tensors -> device\n",
    "    inputs = {\n",
    "        \"input_ids\": packed[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": packed[\"attention_mask\"].to(device),\n",
    "    }\n",
    "\n",
    "    # Build labels: mask everything before the answer starts\n",
    "    ans_ids = processor(text=clean_caption, return_tensors=\"pt\", add_special_tokens=False).input_ids[0].to(device)\n",
    "    full_ids = inputs[\"input_ids\"][0]\n",
    "    start = _find_subseq_start(full_ids, ans_ids)\n",
    "    if start < 0:\n",
    "        # Fallback: compute prompt length using the same call shape (with image)\n",
    "        tmp = processor(text=prompt, images=[image_pil], return_tensors=\"pt\")\n",
    "        start = tmp.input_ids.shape[1]\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[:, :start] = -100  # ignore prompt tokens\n",
    "\n",
    "    # 3) Initialize adversarial pixels in pixel space\n",
    "    adv_px = orig_px.clone()\n",
    "    if random_start:\n",
    "        adv_px = (adv_px + (2*torch.rand_like(adv_px)-1) * epsilon).clamp(0, 1)\n",
    "    adv_px.requires_grad_(True)\n",
    "\n",
    "    # Pre-cast mean/std to model dtype for loop\n",
    "    mean_ = mean_.to(dtype=model.dtype)\n",
    "    std_  = std_.to(dtype=model.dtype)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Normalize on-the-fly for forward pass\n",
    "        norm_adv = (adv_px - mean_) / std_\n",
    "        outputs = model(pixel_values=norm_adv, **inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        grad = torch.autograd.grad(loss, adv_px, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            adv_px += alpha * grad.sign()                # L∞ step in pixel space\n",
    "            delta = (adv_px - orig_px).clamp(-epsilon, epsilon)  # project to ε-ball\n",
    "            adv_px = (orig_px + delta).clamp(0, 1)       # clip to valid pixel range\n",
    "\n",
    "        adv_px.requires_grad_(True)\n",
    "\n",
    "        if verbose and (step % 5 == 0 or step == num_steps-1):\n",
    "            # Sanity prints\n",
    "            print(f\"[Step {step}] loss={loss.item():.4f} \"\n",
    "                  f\"grad_norm={grad.norm().item():.4f} \"\n",
    "                  f\"delta_max={delta.abs().max().item():.5f}\")\n",
    "\n",
    "    # 4) Convert to PIL and get adversarial caption\n",
    "    adv_np = adv_px[0].detach().to(dtype=torch.float32).cpu().permute(1,2,0).numpy()\n",
    "    adv_pil = Image.fromarray((adv_np * 255).clip(0,255).astype(np.uint8))\n",
    "    adv_pil = adv_pil.resize(image_pil.size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        final = processor(text=prompt, images=[adv_pil], return_tensors=\"pt\")\n",
    "        final = {k: v.to(device) for k, v in final.items()}\n",
    "        out_ids = model.generate(**final, max_new_tokens=120)\n",
    "        adv_caption = processor.batch_decode(out_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    return adv_pil, adv_caption, clean_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "201323b4-bbab-4ef9-b5b9-eefde08d66c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7812fa8423b948e483047acf5a6c605d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c031c2a099bd4ebb9a10fadd7cae14d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0] loss=0.8054 grad_norm=43.6250 delta_max=0.06274\n",
      "[Step 5] loss=1.1848 grad_norm=5.5273 delta_max=0.06274\n",
      "[Step 10] loss=1.3706 grad_norm=15.0234 delta_max=0.06274\n",
      "[Step 15] loss=1.4939 grad_norm=7.0391 delta_max=0.06274\n",
      "[Step 20] loss=1.5823 grad_norm=11.4375 delta_max=0.06274\n",
      "[Step 25] loss=1.6233 grad_norm=12.1172 delta_max=0.06274\n",
      "[Step 30] loss=1.6535 grad_norm=6.3594 delta_max=0.06274\n",
      "[Step 35] loss=1.6742 grad_norm=6.4609 delta_max=0.06274\n",
      "[Step 40] loss=1.6339 grad_norm=11.8594 delta_max=0.06274\n",
      "[Step 45] loss=1.6535 grad_norm=9.2656 delta_max=0.06274\n",
      "[Step 50] loss=1.6817 grad_norm=8.7344 delta_max=0.06274\n",
      "[Step 55] loss=1.7111 grad_norm=6.3516 delta_max=0.06274\n",
      "[Step 60] loss=1.7314 grad_norm=3.9590 delta_max=0.06274\n",
      "[Step 65] loss=1.7222 grad_norm=3.8691 delta_max=0.06274\n",
      "[Step 70] loss=1.7595 grad_norm=7.2539 delta_max=0.06274\n",
      "[Step 75] loss=1.7800 grad_norm=7.3359 delta_max=0.06274\n",
      "[Step 80] loss=1.7896 grad_norm=4.2852 delta_max=0.06274\n",
      "[Step 85] loss=1.7825 grad_norm=2.2734 delta_max=0.06274\n",
      "[Step 90] loss=1.8034 grad_norm=5.2266 delta_max=0.06274\n",
      "[Step 95] loss=1.7882 grad_norm=7.3672 delta_max=0.06274\n",
      "[Step 99] loss=1.7990 grad_norm=7.2344 delta_max=0.06274\n",
      "Clean: Describe this image in detail.\n",
      "\n",
      "The image features a woman standing in front of a red curtain, holding a cell phone in her hand. She appears to be singing into a microphone, capturing the attention of the audience. The woman is wearing a necklace, adding a touch of elegance to her performance. The scene is set in a room with a red backdrop, creating a dramatic atmosphere for the performance.\n",
      "Adversarial: Describe this image in detail.\n",
      "\n",
      "The image features a person with a mouth open, possibly singing or yelling. The person is in the center of the scene, and their mouth is open wide, with their tongue sticking out. The person appears to be the main focus of the image.\n",
      "Saved adversarial image to: /workspace/adv_example.jpg\n"
     ]
    }
   ],
   "source": [
    "# --- Load model/processor with /workspace cache ---\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, cache_dir=\"/workspace/hf_cache\")\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None,\n",
    "    cache_dir=\"/workspace/hf_cache\"\n",
    ").to(device).eval()\n",
    "\n",
    "# --- Point to an image under /workspace ---\n",
    "image_path = \"val2014/COCO_val2014_000000290734.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# --- Run attack ---\n",
    "adv_img, adv_cap, clean_cap = pgd_attack_image_llava(\n",
    "    model, processor, image,\n",
    "    prompt=\"<image>\\nDescribe this image in detail.\",\n",
    "    epsilon=16/255,\n",
    "    alpha=1/255,\n",
    "    num_steps=100,\n",
    "    device=device,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Clean:\", clean_cap)\n",
    "print(\"Adversarial:\", adv_cap)\n",
    "\n",
    "# Save the adversarial image to /workspace\n",
    "adv_out = \"/workspace/adv_example.jpg\"\n",
    "adv_img.save(adv_out)\n",
    "print(f\"Saved adversarial image to: {adv_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a172d22-9dbf-4ef0-b908-8bf0853e3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_out = \"clean_example.jpg\"\n",
    "image.save(orig_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4a546-0a6a-4cab-952d-654525cf5291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
